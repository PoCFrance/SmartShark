{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDoS_ML_Sup-V1.6\n",
    "log 1.6: CFM -> ds_preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de différentes étapes:\n",
    "\n",
    "\n",
    "### A - Créer une Dataset \n",
    "##### * Analyser les information requise\n",
    "##### * Analyser la quantiter de paquets moyennés nécéssaire dans chaque info\n",
    "#### * Créer une pré-dataset\n",
    "#### * Transformer la pré-dataset en dataset final\n",
    "\n",
    "### B - Créer un réseau de neurones convenable\n",
    "##### * Analyser le nombre d'entrée\n",
    "##### * Entrainer les neurones\n",
    "##### * Analyser les résultats\n",
    "##### * Ajuster\n",
    "\n",
    "### C - Analyser les résultat de l'IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### A - 1 - Créer une Dataset\n",
    "- Poids moyen des paquets\n",
    "- Delta time moyen entre chaque paquets\n",
    "- Pourcentage d'uttilisation de chaque protocole\n",
    "- Nombre de paquets par seconde\n",
    "- Pourcentage de fois la meme IP src/dst\n",
    "- Pourcentage de fois le meme port src/dst\n",
    "\n",
    "### A - 2 - Créer une Dataset\n",
    "1er éssaie: Un éssai sur des groupes de 100 paquets\n",
    "1.5: Essaie sur 3 comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - 3 - Créer une Dataset\n",
    "\n",
    "#### Note:\n",
    "Pourquoi pas rajouter l'écart moyen entre la len moyenne ?\n",
    "Pourquoi pas rajouter le timeflow de chaque ?\n",
    "\n",
    "###### Pré-Dataset:\n",
    "Timestamp | Delta time | Len paquets fwd | Len paquets bwd | Ip src | port src | Ip dst | Port dst | Protocole | Label\n",
    "\n",
    "\n",
    "###### Dataset final:\n",
    "   Delta Time | Len paquets Fwd| Len paquets Bwd | Paquet/s | % meme Ip src | % meme port src | % meme Ip dst | % meme port dst |  totalité protocole\n",
    "\n",
    "###### Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_epochs = 3\n",
    "w_batch_size = 1 #to add\n",
    "\n",
    "percentage_for_test_data = 0.5\n",
    "reduction_facteur = 0\n",
    "size_list = 20\n",
    "\n",
    "path_ModelCheckpoint = \"models_saved/model-{epoch:03d}-batche_size=20.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Création et utilisation de: ds_create.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation de la dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_benign_csv = \"/run/media/Thytu/TOSHIBA EXT/PoC/Smartshark/DS/V2/Benigns/ds_flow1_900k_batch_size_20.csv\"\n",
    "path_to_ddos_csv = \"/run/media/Thytu/TOSHIBA EXT/PoC/Smartshark/DS/V2/DDoS/ds_ddos_900k_D1_batch_size_20.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoir une idée de la dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_csv = pd.read_csv(path_to_benign_csv, names=[\"Delta time\", \"Len\", \"Protocol\", \"Total Delta\", \"Total Len\", \"Average Delta\", \"Average Len\", \"Delta Std\", \"Len Std\"], dtype='float')\n",
    "ddos_csv = pd.read_csv(path_to_ddos_csv, names=[\"Delta time\", \"Len\", \"Protocol\", \"Total Delta\", \"Total Len\", \"Average Delta\", \"Average Len\", \"Delta Std\", \"Len Std\"], dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benign_csv) > len(ddos_csv):\n",
    "    benign_csv = benign_csv[:len(ddos_csv)]\n",
    "else:\n",
    "    ddos_csv = ddos_csv[:len(benign_csv)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta time</th>\n",
       "      <th>Len</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Total Delta</th>\n",
       "      <th>Total Len</th>\n",
       "      <th>Average Delta</th>\n",
       "      <th>Average Len</th>\n",
       "      <th>Delta Std</th>\n",
       "      <th>Len Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000312</td>\n",
       "      <td>320.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>4447.0</td>\n",
       "      <td>1.348785e-08</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>264.906893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>0.001942</td>\n",
       "      <td>90.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.014081</td>\n",
       "      <td>4537.0</td>\n",
       "      <td>1.564563e-08</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>256.791136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.004135</td>\n",
       "      <td>72.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>4609.0</td>\n",
       "      <td>2.024007e-08</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>252.787778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000279</td>\n",
       "      <td>324.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.018495</td>\n",
       "      <td>4933.0</td>\n",
       "      <td>2.055005e-08</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>249.472109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000158</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>5033.0</td>\n",
       "      <td>2.072558e-08</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>243.364847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Delta time    Len  Protocol  Total Delta  Total Len  Average Delta  \\\n",
       "899995    0.000312  320.0      17.0     0.012139     4447.0   1.348785e-08   \n",
       "899996    0.001942   90.0      17.0     0.014081     4537.0   1.564563e-08   \n",
       "899997    0.004135   72.0      17.0     0.018216     4609.0   2.024007e-08   \n",
       "899998    0.000279  324.0      17.0     0.018495     4933.0   2.055005e-08   \n",
       "899999    0.000158  100.0      17.0     0.018653     5033.0   2.072558e-08   \n",
       "\n",
       "        Average Len  Delta Std     Len Std  \n",
       "899995     0.004941   0.000865  264.906893  \n",
       "899996     0.005041   0.000845  256.791136  \n",
       "899997     0.005121   0.000866  252.787778  \n",
       "899998     0.005481   0.001133  249.472109  \n",
       "899999     0.005592   0.001115  243.364847  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign_csv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta time</th>\n",
       "      <th>Len</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Total Delta</th>\n",
       "      <th>Total Len</th>\n",
       "      <th>Average Delta</th>\n",
       "      <th>Average Len</th>\n",
       "      <th>Delta Std</th>\n",
       "      <th>Len Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>271.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>7.111151e-10</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>67.233788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>271.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>4571.0</td>\n",
       "      <td>1.052227e-09</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>65.287076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>271.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>4842.0</td>\n",
       "      <td>1.054448e-09</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>63.498467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>271.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>5113.0</td>\n",
       "      <td>1.125558e-09</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>61.847907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>271.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>5384.0</td>\n",
       "      <td>1.126668e-09</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>60.318683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Delta time    Len  Protocol  Total Delta  Total Len  Average Delta  \\\n",
       "899995    0.000001  271.0      17.0     0.000640     4300.0   7.111151e-10   \n",
       "899996    0.000307  271.0      17.0     0.000947     4571.0   1.052227e-09   \n",
       "899997    0.000002  271.0      17.0     0.000949     4842.0   1.054448e-09   \n",
       "899998    0.000064  271.0      17.0     0.001013     5113.0   1.125558e-09   \n",
       "899999    0.000001  271.0      17.0     0.001014     5384.0   1.126668e-09   \n",
       "\n",
       "        Average Len  Delta Std    Len Std  \n",
       "899995     0.004778   0.000089  67.233788  \n",
       "899996     0.005079   0.000087  65.287076  \n",
       "899997     0.005380   0.000105  63.498467  \n",
       "899998     0.005681   0.000103  61.847907  \n",
       "899999     0.005982   0.000100  60.318683  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddos_csv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation finale de la dataset\n",
    "#### Mélange des deux dataset pour en former plus qu'une seul et création des labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la DS de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_results(dataset, label, test_dataset, label_test):\n",
    "    x = True if len(dataset) == len(label) else False\n",
    "    print(f\"Is {len(dataset)} equale to {len(label)} ?  {x}\")\n",
    "    x = True if len(test_dataset) == len(label_test) else False\n",
    "    print(f\"Is {len(test_dataset)} equale to {len(label_test)} ?  {x}\")\n",
    "\n",
    "def Creating_splited_datasets(benign_csv, ddos_csv, percentage_for_test_data, reduction_facteur=0):\n",
    "    benign_csv = benign_csv.values.tolist()\n",
    "    ddos_csv = ddos_csv.values.tolist()\n",
    "    \n",
    "    dataset = benign_csv[:int(len(benign_csv) * (1 - reduction_facteur))]\n",
    "    test_dataset = dataset[int(len(dataset) * (1 - percentage_for_test_data)):]\n",
    "    label = [0] * int(len(dataset) * (1 - percentage_for_test_data))\n",
    "    label_test = [0] * len(dataset[int(len(dataset) * (1 - percentage_for_test_data)):])\n",
    "    dataset = dataset[:int(len(dataset) * (1 - percentage_for_test_data))]\n",
    "\n",
    "    print_test_results(dataset, label, test_dataset, label_test)\n",
    "\n",
    "    tmp = ddos_csv[:int(len(ddos_csv) * (1 - reduction_facteur))]\n",
    "    tmp_test = tmp[int(len(tmp) * (1 - percentage_for_test_data)):]\n",
    "    label += [1] * int(len(tmp) * (1 - percentage_for_test_data))\n",
    "    label_test += [1] * int(len(tmp_test))\n",
    "    tmp = tmp[:int(len(tmp) * (1 - percentage_for_test_data))]\n",
    "    \n",
    "    dataset += tmp\n",
    "    test_dataset += tmp_test\n",
    "    \n",
    "    print_test_results(dataset, label, test_dataset, label_test)\n",
    "    \n",
    "    return dataset, label, test_dataset, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 450000 equale to 450000 ?  True\n",
      "Is 450000 equale to 450000 ?  True\n",
      "Is 900000 equale to 900000 ?  True\n",
      "Is 900000 equale to 900000 ?  True\n",
      "0 1\n",
      "900000 900000\n",
      "900000 900000\n"
     ]
    }
   ],
   "source": [
    "dataset, label, test_dataset, test_label = Creating_splited_datasets(benign_csv, ddos_csv, percentage_for_test_data, reduction_facteur)\n",
    "\n",
    "print(label[0], label[len(label) - 1])\n",
    "print(len(label), len(dataset))\n",
    "print(len(test_label), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diminiuer l'effet des grands nombres pour facilité l'apprentissage du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "test_dataset = scaler.fit_transform(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation de la list en np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "900000\n",
      "900000\n",
      "900000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "label = np.array(label)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "print(label); print(test_label)\n",
    "print(len(label)); print(len(dataset))\n",
    "print(len(test_label)); print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise en séries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listing_ds(dataset, label, size_list=20):\n",
    "    result = []\n",
    "    tmp = []\n",
    "    r_label = []\n",
    "    r = 0\n",
    "    for index, data in enumerate(dataset):\n",
    "        tmp.append(data)\n",
    "        r = r + 1 if label[index] == 0 else r - 1\n",
    "        if index % size_list == size_list - 1:\n",
    "            result.append(np.array(tmp))\n",
    "            if r > 0:\n",
    "                r_label.append(0)\n",
    "            else:\n",
    "                r_label.append(1)\n",
    "            tmp = []\n",
    "            r = 0\n",
    "    return np.array(result), np.array(r_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000\n",
      "900000\n",
      "(45000, 20, 9) 45000\n",
      "(45000, 20, 9) 45000\n"
     ]
    }
   ],
   "source": [
    "print(len(label))\n",
    "print(len(test_label))\n",
    "dataset, label = listing_ds(dataset, label)\n",
    "test_dataset, test_label = listing_ds(test_dataset, test_label, size_list)\n",
    "\n",
    "print(dataset.shape, len(label))\n",
    "print(test_dataset.shape, len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(9, input_shape=dataset[0].shape, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(9, input_shape=dataset[0].shape, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Callbacks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_path, histogram_freq=1)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    path_ModelCheckpoint, monitor='val_acc', mode='max', save_best_only=False, verbose=0, #ATTENTION !!\n",
    "    save_weights_only=False, save_freq='epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset, label = shuffle(dataset, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 45000\n",
      "(45000, 20, 9)\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset), len(label))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "    2/45000 [..............................] - ETA: 1:58:51 - loss: 0.7010 - accuracy: 0.5000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.152781). Check your callbacks.\n",
      "45000/45000 [==============================] - 353s 8ms/step - loss: 0.4924 - accuracy: 0.7610 - val_loss: 0.2073 - val_accuracy: 0.9100\n",
      "Epoch 2/3\n",
      "45000/45000 [==============================] - 362s 8ms/step - loss: 0.0581 - accuracy: 0.9915 - val_loss: 2.6546 - val_accuracy: 0.7944\n",
      "Epoch 3/3\n",
      " 1113/45000 [..............................] - ETA: 4:47 - loss: 0.0397 - accuracy: 0.9928"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-25c2ee3ae5a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m--> 847\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    303\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3438\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_median_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3439\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3440\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, label, epochs=w_epochs, batch_size=1, validation_data=(test_dataset, test_label), callbacks=[tensorboard_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultat sous graphique du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_curve = history.history[\"loss\"]\n",
    "acc_curve = history.history[\"accuracy\"]\n",
    "\n",
    "plt.plot(loss_curve)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc_curve)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation of data to evaluate false positives number on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives_test = scaler.fit_transform(benign_csv.values.tolist())\n",
    "#false_positives_test = scaler.fit_transform(benign_csv[0:-1])\n",
    "#false_positives_label = np.array([0] * len(false_positives_test))\n",
    "false_positives_label = [0] * len(false_positives_test)\n",
    "\n",
    "false_positives_test, false_positives_label = listing_ds(false_positives_test, false_positives_label, size_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of false positives number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 231.6558 - accuracy: 0.1480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[231.6558380126953, 0.14800000190734863]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(false_positives_test[0:1000], false_positives_label[0:1000], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 231.6558 - accuracy: 0.1480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[231.6558380126953, 0.14800000190734863]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aze = benign_csv.values.tolist()\n",
    "print(type(aze))\n",
    "aze = scaler.fit_transform(aze)\n",
    "print(type(aze))\n",
    "qwe = [0] * len(aze)\n",
    "aze, qwe = listing_ds(aze, qwe, size_list)\n",
    "model.evaluate(aze[0:1000], qwe[0:1000], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stest(dataset, labels):\n",
    "    r = []\n",
    "    rl = []\n",
    "    for index, data in enumerate(dataset):\n",
    "        if labels[index] == 0:\n",
    "            r.append(data)\n",
    "            rl.append(0)\n",
    "    return np.array(r), np.array(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07937393942999518\n",
      "-0.6212217280407947\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.008166040293872356, 1.0]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, rl = stest(dataset, label)\n",
    "tr, trl = stest(test_dataset, test_label)\n",
    "tt = np.concatenate((r, tr))\n",
    "ttl = np.concatenate((rl, trl))\n",
    "\n",
    "print(r[0][0][0]);print(false_positives_test[0][0][0]);print(type(r));print(type(false_positives_test))\n",
    "model.evaluate(tt[0:1000], ttl[0:1000], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(false_positives_test))\n",
    "def know_perc_of_each(labels):\n",
    "    b = 0\n",
    "    d = 0\n",
    "    for label in labels:\n",
    "        if label == 0:\n",
    "            b += 1\n",
    "        else:\n",
    "            d += 1\n",
    "    print(f\"There is {int(b * 100 / len(labels))}% of benign and {int(d * 100 / len(labels))}% of ddos\")\n",
    "know_perc_of_each(label)\n",
    "know_perc_of_each(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation of data to evaluate false negatives number on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#false_negatives_test = scaler.fit_transform(ddos_csv[len(ddos_csv) - 1_000_000:])\n",
    "false_negatives_test = scaler.fit_transform(ddos_csv[0:-1])\n",
    "false_negatives_label = np.array([1] * len(false_negatives_test))\n",
    "#false_negatives_label = np.array([1] * 1_000_000)\n",
    "\n",
    "false_negatives_test, false_negatives_label = listing_ds(false_negatives_test, false_negatives_label, size_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of false negatives number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(false_negatives_test, false_negatives_label, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(false_positives_test.shape)\n",
    "print(false_negatives_test.shape)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(w_epochs):\n",
    "    if epoch + 1 < 10:\n",
    "        file_path = f\"models_saved/model-00{epoch + 1}-batche_size=20.h5\"\n",
    "    else:\n",
    "        file_path = f\"models_saved/model-0{epoch + 1}-batche_size=20.h5\"\n",
    "    tmp_model = tf.keras.models.load_model(file_path)\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"False Positives: \")\n",
    "    tmp_model.evaluate(false_positives_test, false_positives_label, batch_size=1)\n",
    "    print(\"False Negatives: \")\n",
    "    tmp_model.evaluate(false_negatives_test, false_negatives_label, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(test_dataset, test_label, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(false_negatives_test[:1000], false_negatives_label[0:1000], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 104s 2ms/step - loss: 0.2073 - accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2072926014661789, 0.9099555611610413]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model = tf.keras.models.load_model(\"models_saved/model-001-batche_size=20.h5\")\n",
    "tmp_model.evaluate(test_dataset, test_label, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
